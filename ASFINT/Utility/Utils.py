import numpy as np
import pandas as pd
from collections.abc import Iterable
from typing import Dict, Any
from pathlib import Path
# import spacy
# nlp_model = spacy.load("en_core_web_md")
from sklearn.metrics.pairwise import cosine_similarity 
# from rapidfuzz import fuzz, process

from ASFINT.Utility.Cleaning import is_type, in_df, any_in_df, is_valid_iter, any_drop

def ensure_folder(path):
    """
    Ensure a folder exists. Accepts either str or Path.
    """
    path = Path(path)  # normalize
    if not path.exists():
        print(f"Creating missing folder: {path}")
        path.mkdir(parents=True, exist_ok=True)
    return path

def column_converter(df:pd.DataFrame, 
                    dict: Dict = None, 
                    cols: Iterable = None, 
                    t: Any = None, 
                    fillna_val: Any = np.nan, 
                    mutate: bool = False, 
                    date_varies: pd.Timestamp = False):
    """
    Converts columns in a DataFrame to a given type.

    Supports:
        - single column type conversion (`cols` + `t`)
        - batch conversion using a dict: {type_str: [col1, col2, ...]}

    Valid types: 'int', 'float', 'str', 'timestamp' (for pd.Timestamp)

    Args:
        df: Input DataFrame
        dict: Optional dict of types to columns
        cols: List of columns to convert (used with `t`)
        t: Type to convert `cols` to
        fillna_val: Value to fill NaNs with after coercion
        mutate: If True, mutates `df` in-place; else returns a copy
        date_varies: Set True to handle mixed datetime formats per-cell

    Returns:
        DataFrame with converted columns

    Version 2.0: CAN Convert multple columns to different types via dictionary input
    """
    TYPE_MAP = {
    "int": int,
    "float": float,
    "str": str,
    "timestamp": pd.Timestamp,  
    }
    assert cols and t or not cols and not t, f"If 'cols' arg is specified so too must the 't' arg be specified."
    copy = df.copy()
    if dict:
        for dtype_str, columns in dict.items():
            assert dtype_str in TYPE_MAP, f"Datatype '{dtype_str}' not supported. Choose from {list(TYPE_MAP.keys())}"
            _column_converter(copy, cols=columns, t=TYPE_MAP[dtype_str], fillna_val=fillna_val, mutate=True, date_varies=date_varies)
    if cols and t:
        _column_converter(copy, cols=cols, t=t, fillna_val=fillna_val, mutate=True, date_varies=date_varies)
    return copy
    
def _column_converter(df, cols, t, fillna_val = np.nan, mutate = False, date_varies = False):
    
    if fillna_val is None:
        fillna_val = np.nan

    if isinstance(cols, str): # If a single column is provided, convert to list for consistency
        cols = [cols]
    
    if not mutate:
        df = df.copy()

    if t == int:
        if pd.isna(fillna_val):
            fillna_val = -1
        assert isinstance(fillna_val, int), f"Trying to convert columns to type int but 'fillna_val' is type {type(fillna_val)} rather than int"
        df[cols] = df[cols].apply(pd.to_numeric, errors='coerce').fillna(fillna_val).astype(int)
        
    elif t == float:
        if pd.isna(fillna_val):
            fillna_val = 0.0
        assert isinstance(fillna_val, float), f"Trying to convert columns to type float but 'fillna_val' is type {type(fillna_val)} rather than float"
        df[cols] = df[cols].apply(pd.to_numeric, errors='coerce').fillna(fillna_val)
        
    elif t == pd.Timestamp:
        if not date_varies:
            for col in cols: 
                df[col] = pd.to_datetime(df[col], errors='coerce')
        else: #to handle different entries being formatted differently
            for col in cols: 
                for index in df[col].index:
                    try:
                        df.loc[index, col] = pd.Timestamp(df.loc[index, col])
                    except ValueError as e:
                        df.loc[index, col] = pd.NaT
                df[col] = pd.to_datetime(df[col], errors='coerce') # sets the column's dtype to datetime64
        
    elif t == str:
        df[cols] = df[cols].astype(str).fillna(fillna_val)
        
    else:
        try:
            df[cols] = df[cols].astype(t).fillna(fillna_val)
        except Exception as e:
            print(f"Error converting {cols} to {t}: {e}")
    
    if not mutate:
        return df

def column_renamer(df, rename):
        """
        Renames columns of a df. 'rename' argument can handle keywords for special ASUC CSVs. Only keyword currently implemented is 'OASIS-Standard'.
        Can handle extra columns. They just don't get renamed if they aren't explicitly named in the 'renamed' arg.
        This function is used to standardize and routinize renaming raw files of fixed formats that are regularly ecnountered such as the OASIS Club Registration spreadsheet files. 
        """
        cleaned_df = df.copy()
        cols = cleaned_df.columns

        if rename == 'OASIS-Standard': #proceed with standard renaming scheme
            cleaned_df = cleaned_df.rename(columns={
                cols[2] : 'Reg Steps Complete',
                cols[3] : 'Reg Form Progress',
                cols[4] : 'Num Signatories',
                cols[9] : 'OASIS Center Advisor'
            }
            )
        else:
            #rename should be a dictionary of indexes to the renamed column
            assert isinstance(rename, dict), 'rename must be a dictionary mapping the index of columns/names of columns to rename to their new names'
            assert in_df(list(rename.keys()), df), 'names or indices of columns to rename must be in given df'
            if is_type(list(rename.keys()), int):
                cleaned_df = cleaned_df.rename(columns={ cols[key] : rename[key] for key in rename.keys()})
            elif is_type(list(rename.keys()), str):
                cleaned_df = cleaned_df.rename(columns={ key : rename[key] for key in rename.keys()})
        
        cleaned_df.columns = cleaned_df.columns.str.strip() #removing spaces from column names
        assert is_type(cleaned_df.columns, str), 'CRU Final Check: columns not all strings'

        return cleaned_df


def oasis_cleaner(OASIS_master, approved_orgs_only=True, year=None, club_type=None):
    """
    Cleans the OASIS master dataset by applying filters and removing unnecessary columns.

    Version 2.0: Updated with cleaning functions from this package

    Parameters:
    OASIS_master (DataFrame): The master OASIS dataset to be cleaned.
    approved_orgs_only (bool): If True, only includes active organizations (where 'Active' == 1).
    year (int, float, str, or list, optional): The year(s) to filter by. Can be:
        - A single academic year as a string (e.g., '2023-2024').
        - A single year rank as an integer or float (e.g., 2023 or 2023.0).
        - A list of academic years or year ranks.
    club_type (str, optional): Filters by a specific club type from the 'OASIS RSO Designation' column.

    Returns:
    DataFrame: The cleaned OASIS dataset with the specified filters applied and unnecessary columns removed.

    Notes:
    - When filtering by year:
        * Strings are matched against the 'Year' column (e.g., '2023-2024').
        * Integers or floats are matched against the 'Year Rank' column (e.g., 2023).
    - The following columns are always dropped: 'Orientation Attendees', 'Spring Re-Reg. Eligibility', 
        'Completed T&C', 'Num Signatories', 'Reg Form Progress', and 'Reg Steps Complete'.

    Raises:
    TypeError: If the year is not a string, integer, float, or list of these types.
    AssertionError: If a float in the year list is not an integer (e.g., 2023.5).
    """
    if year is not None:
        assert is_type(year, (str, int, float)), "Year must be a string, integer, float, or a tuple, list or pd.Series of these types."
        if isinstance(year, (str, int, float)):
            year = [year]
        if is_type(year, float):
            for y in year:
                if y != round(y):
                    raise AssertionError("All floats in `year` must represent integers.")

    OASISCleaned = OASIS_master.copy()
    assert in_df(['Active', 'Year', 'Year Rank', 'OASIS RSO Designation'], OASIS_master), "'Year', 'Year Rank', 'Active' or 'OASIS RSO Designation' columns not found in inputted OASIS dataset."
    if approved_orgs_only:
        OASISCleaned = OASISCleaned[OASISCleaned['Active'] == 1]

    if year is not None:
        if is_type(year, str): #at this point year should be an iterable
            OASISCleaned = OASISCleaned[OASISCleaned['Year'].isin(year)]
        elif is_type(year, int) or is_type(year, float):
            OASISCleaned = OASISCleaned[OASISCleaned['Year Rank'].isin(year)]

    if club_type is not None:
            OASISCleaned = OASISCleaned[OASISCleaned['OASIS RSO Designation'] == club_type]
    
    standard_drop_cols = ['Orientation Attendees', 'Spring Re-Reg. Eligibility', 'Completed T&C', 'Num Signatories', 'Reg Form Progress', 'Reg Steps Complete']
    if any_in_df(standard_drop_cols, OASISCleaned):
        OASISCleaned = any_drop(OASISCleaned, standard_drop_cols)
    return OASISCleaned

# def sucont_cleaner(df, year):
#     """Version 1.0: Just handles cleaning for years"""
#     assert ('Date' in df.columns) and is_type(df['Date'], pd.Timestamp), 'df must have "Date" column that contains only pd.Timestamp objects'
    
#     copy = df.copy()
#     year_range = reverse_academic_year_parser(year)
#     mask = (copy['Date'] >= year_range[0]) & (copy['Date'] <= year_range[1])
#     return pd.DataFrame(copy[mask])

def _get_loc_wrapper(df, index_iter, elem=None):
        """returns numerical index or list of numerical indices corresponding to a non-numerical index or list of non-numerical indices
        index_iter: the instance or list of non-numerical indices to be converted into numerical integer indices"""
        
        if isinstance(index_iter, Iterable) and not isinstance(index_iter, (str, bytes)):
            assert all(index_iter.isin(df.index)), f"Not all entries in 'index_iter' under 'heading_finder' > '_get_loc_wrapper' are found in inputted df.index : {df.index}"
        
        if isinstance(index_iter, int):
            return df.index.get_loc(index_iter)
        else: 
            index_iter = pd.Series(index_iter)
            try:
                print(f"get col index itter: {index_iter}")
                indices = pd.Series(index_iter).apply(lambda x: df.index.get_loc(x)).sort_values()
                if indices.empty:
                    raise ValueError('_get_loc_wrapper indices negative')
                if elem is None: 
                    return indices
                else: 
                    assert is_type(elem, int), "Inputted 'elem' arg must be int or list of int specifyig indices to extract."
                    return indices[elem]
            except Exception as e:
                raise e

def heading_finder(df, start_col, start, nth_start = 0, shift = 0, start_logic = 'exact', end_col = None, end = None, nth_end = 0, end_logic = 'exact') -> pd.DataFrame:
    """
    Non-destructively adjusts the DataFrame to start at the correct header. Can also specify where to end the new outputted dataframe.
    Last two arguments 'start_logic' and 'end_logic' allow for 'exact' or 'contains' matching logics for the header value specified in 'start' and the ending value in 'end' we're looking for.
    TLDR this is a fansy pands loc/iloc wrapper. 

    Parameters:
    - df (pd.DataFrame): The input DataFrame.
    - start_col (str or int): Column index or name to search for the header.
    - start (str): The name of the header/string to look for in the 'start_col' column where we want to start the new dataframe.
    - nth_start (int): If there are multiple occurences of 'start' in 'start_col', begin our new dataframe at the 'nth_start' occurences of 'start' in 'col'.
    - shift (int, optional): Dictates how many rows below the header row the new dataframe should start at. 
        Default is 0 which means that the extracted start value becomes the header (ie the row corresponding to the 'nth_start match in 'start_col' is set as the header).
    
    - end_col (str or int): Column index or name to search for the ending value.
    - end (str, int, or list, optional): The ending value(s) or row index to limit the DataFrame. 
        The row corresponding to the end value is excluded
    - nth_end (int): If there are multiple occurences of 'end' in 'col' start at the 'nth_start' occurences of 'header' in 'col'.

    - start_logic (str, optional): Matching method for the `start` value. Default is exact matching.
        start logic options implemented: 'exact', 'contains', 'in'.
     - end_logic (str, optional): Matching method for the `end` value.  Default is exact matching.
        ending logic options implemented: 'exact', 'contains', 'in'.

    Returns:
    - pd.DataFrame: The adjusted DataFrame starting from the located header and ending at the specified end.
    """
    assert isinstance(start_col, str) or isinstance(start_col, int), "'start_col' must be index of column or name of column."
    assert in_df(start_col, df), 'Given start_col is not in the given df.'

    if end_col is not None:
        assert isinstance(end_col, str) or isinstance(end_col, int), "'end_col' must be index of column or name of column."
        assert in_df(end_col, df), 'Given end_col is not in the given df.'
    else:
        end_col = start_col

    start_col_index = df.columns.get_loc(start_col) if isinstance(start_col, str) else start_col #extract index of start and end column
    end_col_index = df.columns.get_loc(end_col) if isinstance(end_col, str) else end_col #extract index of start and end column


    # print(f"start_col_index value: {start_col_index}")
    # print(f"Label: {start}")
    # print(f"Inputted index iter: {df[df.iloc[:, start_col_index].str.strip() == start].index}")
    
    if start_logic == 'exact':
        matching_indices: pd.Index = df[df.iloc[:, start_col_index].astype(str).str.strip() == str(start)].index 
    elif start_logic == 'contains':
        matching_indices: pd.Index = df[df.iloc[:, start_col_index].astype(str).str.strip().str.contains(str(start), regex=False, na=False)].index 
    else: 
        raise ValueError("Invalid 'start_logic'. Use 'exact' or 'contains'.") 

    if matching_indices.empty:
        raise ValueError(f"Header '{start}' not found in column '{start_col}'.")

    start_index: int = df.index.get_loc(matching_indices[nth_start]) #select nth_start if multiple matches with header exist

    # print(f"type start: {type(start_index)}")
    # print(f"type shift: {type(shift)}")
    start_index = start_index + shift
    if start_index >= len(df):
        raise ValueError("Shifted start index exceeds DataFrame length.")

    df = df.iloc[start_index:]

    if end is not None:
        if isinstance(end, int):
            if end < len(df):
                return df.iloc[:end]
            raise ValueError("Ending index exceeds the remaining DataFrame length.")

        elif isinstance(end, Iterable) and not isinstance(end, (str, bytes)): # if 'end' is a iterable containing values to end by we want to iterate through it
            pattern = '|'.join(map(str, end))
            if end_logic == 'exact':
                end_matches = df[df.iloc[:, end_col_index].isin(end)].index
            elif end_logic == 'contains':
                end_matches = df[df.iloc[:, end_col_index].fillna('').str.contains(pattern, na=False)].index
            else:
                raise ValueError("Invalid 'end_logic'. Use 'exact' or 'contains'.")
        elif isinstance(end, str):
            if end_logic == 'exact':
                end_matches = df[df.iloc[:, end_col_index] == str(end)].index
            elif end_logic == 'contains':
                end_matches = df[df.iloc[:, end_col_index].fillna('').str.contains(str(end), na=False)].index
            else:
                raise ValueError("Invalid 'end_logic'. Use 'exact' or 'contains'.")
        else: # brute force try to convert dataframe and 'end' input into a string to match
            if end_logic == 'exact':
                end_matches = df[df.iloc[:, end_col_index].astype(str) == str(end)].index
            elif end_logic == 'contains':
                end_matches = df[df.iloc[:, end_col_index].fillna('').astype(str).str.contains(str(end), na=False)].index
            else:
                raise ValueError("Invalid 'end_logic'. Use 'exact' or 'contains'.")

        if not end_matches.empty:
            end_index = df.index.get_loc(end_matches[nth_end])
            rv = df.iloc[:end_index]
            rv_header = df.iloc[0,:]
            rv = rv[1:]
            rv.columns = rv_header.values # so the index of the extracted row doesn't get set as the index label 
            rv = rv.reset_index(drop=True)
            return rv
        raise ValueError(f"End value '{end}' not found in column '{end_col}'.")

    rv = df
    rv_header = df.iloc[0,:]
    rv = rv[1:]
    rv.columns = rv_header.values # so the index of the extracted row doesn't get set as the index label 
    rv = rv.reset_index(drop=True)
    return rv

def ending_keyword_adder(df, given_start = 'Appx', start_col = 0, adding_end_keyword='END', end_col = 0, alphabet=None, reporting=False) -> pd.DataFrame:
    """Non-mutatively adds 'end_keyword' to signify end of a section for FR documents. Also shifts the dataframe down and updates the columns."""
    assert isinstance(start_col, str) or isinstance(start_col, int), "'start_col' must be index of column or name of column."
    assert in_df(start_col, df), f"start_col '{start_col}' is not in df columns: {df.columns.tolist()}"

    if end_col is not None:
        assert isinstance(end_col, str) or isinstance(end_col, int), "'end_col' must be index of column or name of column."
        assert in_df(end_col, df), 'Given end_col is not in the given df.'
    else:
        end_col = start_col

    start_col_index = df.columns.get_loc(start_col) if isinstance(start_col, str) else start_col #extract index of start and end column
    end_col_index = df.columns.get_loc(end_col) if isinstance(end_col, str) else end_col #extract index of start and end column
    
    copy = df.copy()
    copy = heading_finder(copy, start_col=0, start=given_start, start_logic='contains', shift=-1) # no ending logic just take all rows below the starting point
    col = copy.columns[start_col_index]
    ending_row_index = None

    try:
        if alphabet is None:
            na_indices = copy[copy[col].isna()].index
            if na_indices.empty:
                raise ValueError("No NaN row found to mark as end of section.")
            ending_row_index = na_indices[0]
        else:
            valid_rows: pd.DataFrame = copy[copy[col].isin(alphabet)]
            if valid_rows.empty:
                raise ValueError("No valid rows with alphabet keys found.")
            ending_row_index = valid_rows.index[-1] + 1

        if ending_row_index < len(copy):
            copy.iloc[ending_row_index, end_col_index] = adding_end_keyword
    except Exception as e:
           print(f"Warning: Could not insert ending keyword '{adding_end_keyword}' in column {end_col}, received exception\n{e}")
    if reporting:
        print(f"Inserted '{adding_end_keyword}' at row {ending_row_index}, col '{copy.columns[end_col_index]}'")
    return copy
